{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "comp551-p3-group27.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6vgRpLDwn8p"
      },
      "source": [
        "# Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xPI17ezwz-v"
      },
      "source": [
        "Import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g16jnN1Rwh5V"
      },
      "source": [
        "from classifier.network.toy_net import *\n",
        "from classifier.network.alex_net import *\n",
        "from classifier.plugin import *\n",
        "from classifier.metric import *\n",
        "from preprocess import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "281jjrUuwwtp"
      },
      "source": [
        "Paths definition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFXhNuXGxOz2"
      },
      "source": [
        "DATASET_PATH = Path(\"../dataset\")\n",
        "SUBMISSION_PATH = Path(\"submissions\")\n",
        "TRAINED_MODELS_PATH = Path(\"trained-models\")\n",
        "PROCESSED_DATA_PATH = Path(\"processed-data\")\n",
        "WRONG_PRED_ENTRIES_PATH = Path(\"wrong-pred-entries\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM-C7bRExQVD"
      },
      "source": [
        "Optimizer setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rr9rytaxQY7"
      },
      "source": [
        "ADAM_PROFILE = OptimizerProfile(Adam, {\n",
        "            \"lr\": 0.0005,\n",
        "            \"betas\": (0.9, 0.99),\n",
        "            \"eps\": 1e-8\n",
        "        })\n",
        "\n",
        "SGD_PROFILE = OptimizerProfile(SGD, {\n",
        "        'lr': 0.0005,\n",
        "        'momentum': 0.99\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw-X7kEKxeyx"
      },
      "source": [
        "# A toy demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha4oWYj2xhSD"
      },
      "source": [
        "trained_toy_net_PATH = Path(TRAINED_MODELS_PATH / \"toy-net\")\n",
        "def toy_demo():\n",
        "    training_set = read_train_labeled(DATASET_PATH)\n",
        "    validation_proportion = 0.1\n",
        "    validation, train = partition(training_set, [int(len(training_set) * validation_proportion)], True)\n",
        "    toy_clf = ToyClassifier(train, validation)\n",
        "    toy_clf.train(epochs=100,\n",
        "                  batch_size=100,\n",
        "                  plugins=[\n",
        "                      #save_model(trained_toy_net_PATH),\n",
        "                      #calc_train_val_performance(accuracy),\n",
        "                      print_train_val_performance(accuracy),\n",
        "                      #log_train_val_performance(accuracy),\n",
        "                      #save_training_message(trained_toy_net_PATH),\n",
        "                      #save_train_val_performance(trained_toy_net_PATH, accuracy),\n",
        "                      plot_train_val_performance(trained_toy_net_PATH, 'Modified AlexNet', accuracy, True, True)\n",
        "                  ]\n",
        "                  )\n",
        "    #toy_clf = ToyClassifier(train, validation)\n",
        "    #toy_clf.load_network(trained_toy_net_PATH, 2)\n",
        "    print(toy_clf.val_performance(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayP4hbtCxkq4"
      },
      "source": [
        "toy_demo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1JjTW0Txyi6"
      },
      "source": [
        "# Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxA9qmaJyIk0"
      },
      "source": [
        "# path for storing augmented data\n",
        "rotation_augmented_data_PATH = Path(PROCESSED_DATA_PATH / \"rotation-augmented-dataset-last90percent.data\")\n",
        "\n",
        "# path for storing augmented data, images containing x or t are not rotated\n",
        "rotation_augmented_ignorext_data_PATH = Path(PROCESSED_DATA_PATH / \"rotation-augmented-ignorext-dataset-last90percent.data\")\n",
        "\n",
        "# path for all alex nets\n",
        "TRAINED_ALEX_NET_PATH = Path(TRAINED_MODELS_PATH / \"alex-nets\")\n",
        "\n",
        "# path for storing wrong classified images\n",
        "ALEX_NET_WRONG_PREDICTION_PATH = Path(WRONG_PRED_ENTRIES_PATH / \"alex-nets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9lnnSkJ0MAO"
      },
      "source": [
        "def run_alex(n_way: int, depth: Tuple[int, int, int], scaled: bool, relabeled: bool, rotation_augment = False, ignorext = False):\n",
        "    print(f\"n_way: {n_way}\")\n",
        "    print(f\"depth: {depth}\")\n",
        "    training_set = read_train_labeled(DATASET_PATH)\n",
        "    validation_proportion = 0.1\n",
        "    trained_alex_net_PATH = TRAINED_ALEX_NET_PATH\n",
        "    alex_net_rotation_augmented_WRONG_PRED_PATH = ALEX_NET_WRONG_PREDICTION_PATH\n",
        "    if rotation_augment:\n",
        "        trained_alex_net_PATH = Path(Path(str(trained_alex_net_PATH) + \"-rotation-augmented\"))\n",
        "        alex_net_rotation_augmented_WRONG_PRED_PATH = Path(Path(str(alex_net_rotation_augmented_WRONG_PRED_PATH) + \"-rotation-augmented\"))\n",
        "        if ignorext:\n",
        "            trained_alex_net_PATH = Path(Path(str(trained_alex_net_PATH) + \"-rotation-augmented-ignorext\"))\n",
        "            alex_net_rotation_augmented_WRONG_PRED_PATH = Path(\n",
        "                Path(str(alex_net_rotation_augmented_WRONG_PRED_PATH) + \"-rotation-augmented-ignorext\"))\n",
        "\n",
        "        validation, train = partition(training_set, [int(len(training_set) * validation_proportion)], False)\n",
        "        train = torch.load(rotation_augmented_data_PATH)\n",
        "    else:\n",
        "        validation, train = partition(training_set, [int(len(training_set) * validation_proportion)], False)\n",
        "    if scaled:\n",
        "        trained_alex_net_PATH = Path(Path(str(trained_alex_net_PATH) + \"-scaled\"))\n",
        "        alex_net_rotation_augmented_WRONG_PRED_PATH = Path(\n",
        "            Path(str(alex_net_rotation_augmented_WRONG_PRED_PATH) + \"-scaled\"))\n",
        "        train = preprocess_scale_image(train)\n",
        "        validation = preprocess_scale_image(validation)\n",
        "    if relabeled:\n",
        "        trained_alex_net_PATH = Path(Path(Path(str(trained_alex_net_PATH) + \"-relabeled\")))\n",
        "        alex_net_rotation_augmented_WRONG_PRED_PATH = Path(\n",
        "            Path(str(alex_net_rotation_augmented_WRONG_PRED_PATH) + \"-relabeled\"))\n",
        "        train = preprocess_260_labels(train)\n",
        "        validation = preprocess_260_labels(validation)\n",
        "\n",
        "    trained_alex_net_PATH = Path(trained_alex_net_PATH / f\"{n_way}way-depth{depth}\")\n",
        "    alex_net_rotation_augmented_WRONG_PRED_PATH = Path(alex_net_rotation_augmented_WRONG_PRED_PATH / f\"{n_way}way-depth{depth}\")\n",
        "    alex = AlexNetClassifier(train, validation, n_way=n_way, depth=depth, scaled=scaled, relabeled=relabeled)\n",
        "    alex.set_optimizer(ADAM_PROFILE)\n",
        "    alex.train(30,\n",
        "               batch_size=100,\n",
        "               plugins=[\n",
        "                   save_model(trained_alex_net_PATH),\n",
        "                   calc_train_val_performance(accuracy),\n",
        "                   print_train_val_performance(accuracy),\n",
        "                   log_train_val_performance(accuracy),\n",
        "                   save_training_message(trained_alex_net_PATH),\n",
        "                   plot_train_val_performance(trained_alex_net_PATH, 'Modified AlexNet', accuracy, show=False, save=True),\n",
        "                   elapsed_time(),\n",
        "                   save_train_val_performance(trained_alex_net_PATH, accuracy),\n",
        "               ])\n",
        "    alex.extract_wrong_pred_entries(alex_net_rotation_augmented_WRONG_PRED_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6s_nL42KqAZ"
      },
      "source": [
        "Run 2-way networks with different depth of feature extraction in each scale space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlGK1eXIEhsd"
      },
      "source": [
        "run_alex(2, (1,1,2), scaled=False, relabeled=False, rotation_augment=False) # original AlexNet\n",
        "run_alex(2, (1,1,3), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(2, (1,2,2), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(2, (1,2,3), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(2, (2,2,4), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(2, (2,3,3), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(2, (2,3,4), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(2, (3,3,5), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(2, (3,4,4), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(2, (3,4,5), scaled=False, relabeled=False, rotation_augment=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phb61ciNK4Xt"
      },
      "source": [
        "We tested (3,4,4) to be the best. Now try different number of ways"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxew0LQ8K4mS"
      },
      "source": [
        "run_alex(1, (3,4,4), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(3, (3,4,4), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(4, (3,4,4), scaled=False, relabeled=False, rotation_augment=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb8iV_wLIdh"
      },
      "source": [
        "The 1-way model performs almost as well as 2-way. Now confirm that the depth is still the best in 1-way models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHWUk97RLZGj"
      },
      "source": [
        "run_alex(1, (1,1,2), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (1,1,3), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (1,2,2), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (1,2,3), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (2,2,4), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (2,3,3), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (2,3,4), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (3,3,5), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (3,4,4), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (3,4,5), scaled=False, relabeled=False, rotation_augment=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4Rz_LDQMUMc"
      },
      "source": [
        "We use the 1 way version with depths (3,4,4) as our best single model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z17SU_lOLkCq"
      },
      "source": [
        "# Preprocesses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT54zaCcLpv-"
      },
      "source": [
        "Test impact of scaling the image to 112*112 and/or expand the labels of the dataset from 34 zeros with 2 ones to 260 labels via a surjective mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTII4gL-L8fX"
      },
      "source": [
        "run_alex(1, (3,4,4), scaled=False, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (3,4,4), scaled=True, relabeled=False, rotation_augment=False)\n",
        "run_alex(1, (3,4,4), scaled=False, relabeled=True, rotation_augment=False)\n",
        "run_alex(1, (3,4,4), scaled=True, relabeled=True, rotation_augment=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFRN0uMJMkAM"
      },
      "source": [
        "Test impact of augmenting the training data by rotations of +/- 10 20 30 degrees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "payMWQaULp01"
      },
      "source": [
        "# save the preprocessed data\n",
        "validation, train = partition(training_set, [int(len(training_set) * validation_proportion)], False)\n",
        "process_data(train, rotation_augmented_data_PATH, [preprocess_rotate,],\n",
        "                     [{'rotations': [-30, -20, -10, 0, 10, 20, 30]}]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMzZ0-QXNhW_"
      },
      "source": [
        "run_alex(1, (3,4,4), scaled=False, relabeled=False, rotation_augment=True)\n",
        "run_alex(1, (3,4,4), scaled=True, relabeled=False, rotation_augment=True)\n",
        "run_alex(1, (3,4,4), scaled=False, relabeled=True, rotation_augment=True)\n",
        "run_alex(1, (3,4,4), scaled=True, relabeled=True, rotation_augment=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfzN866s9eoK"
      },
      "source": [
        "# Semisupervised Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GCAjm2g9n3R"
      },
      "source": [
        "run_alex(1, (3, 4, 4), scaled=False, relabeled=False, rotation_augment=False, semisupervised=True)\n",
        "run_alex(1, (3, 4, 4), scaled=True, relabeled=False, rotation_augment=False, semisupervised=True)\n",
        "run_alex(1, (3, 4, 4), scaled=False, relabeled=True, rotation_augment=False, semisupervised=True)\n",
        "run_alex(1, (3, 4, 4), scaled=True, relabeled=True, rotation_augment=False, semisupervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYZl8HSGyBIA"
      },
      "source": [
        "# Committee of Classifiers\n",
        "A group of trained classifiers is chosen, and the result is obtained by polling with weights assigned to each classifier.\n",
        "\n",
        "Our best result comes from a committe of 8 classifiers. 4 trained on augmented dataset, 4 trained using semisupervised learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByEoQio9N3V3"
      },
      "source": [
        "# obtain validation accuracy\n",
        "def run_committee_val():\n",
        "    training_set = read_train_labeled(DATASET_PATH)\n",
        "    validation_proportion = 0.1\n",
        "    validation, train = partition(training_set, [int(len(training_set) * validation_proportion)], False)\n",
        "    train_scaled = preprocess_scale_image(train)\n",
        "    train_scaled_relabeled = preprocess_260_labels(train_scaled)\n",
        "    train_relabeled = preprocess_260_labels(train)\n",
        "\n",
        "    validation_scaled = preprocess_scale_image(validation)\n",
        "    validation_scaled_relabeled = preprocess_260_labels(validation_scaled)\n",
        "    validation_relabeled = preprocess_260_labels(validation)\n",
        "\n",
        "    p1 = {'n_way': 1, 'depth': (3, 4, 4), 'scaled': False, 'relabeled': False}\n",
        "    p2 = {'n_way': 1, 'depth': (3, 4, 4), 'scaled': True, 'relabeled': False}\n",
        "    p3 = {'n_way': 1, 'depth': (3, 4, 4), 'scaled': False, 'relabeled': True}\n",
        "    p4 = {'n_way': 1, 'depth': (3, 4, 4), 'scaled': True, 'relabeled': True}\n",
        "\n",
        "    original = Path(TRAINED_MODELS_PATH / 'alex-nets-rotation-augmented' / '1way-depth(3, 4, 4)')\n",
        "    scaled = Path(TRAINED_MODELS_PATH / 'alex-nets-rotation-augmented-scaled' / '1way-depth(3, 4, 4)')\n",
        "    relabeled = Path(TRAINED_MODELS_PATH / 'alex-nets-rotation-augmented-relabeled' / '1way-depth(3, 4, 4)')\n",
        "    scaled_relabeled = Path(\n",
        "        TRAINED_MODELS_PATH / 'alex-nets-rotation-augmented-scaled-relabeled' / '1way-depth(3, 4, 4)')\n",
        "\n",
        "    alex1 = AlexNetClassifier(train, validation, **p1)\n",
        "    alex2 = AlexNetClassifier(train_scaled, validation_scaled, **p2)\n",
        "    alex3 = AlexNetClassifier(train_relabeled, validation_relabeled, **p3)\n",
        "    alex4 = AlexNetClassifier(train_scaled_relabeled, validation_scaled_relabeled, **p4)\n",
        "    alex1.load_network(original, 28)\n",
        "    alex2.load_network(scaled, 30)\n",
        "    alex3.load_network(relabeled, 7)\n",
        "    alex4.load_network(scaled_relabeled, 28)\n",
        "    pred1 = Tensor([]).to(alex1.device)\n",
        "    pred2 = Tensor([]).to(alex2.device)\n",
        "    pred3 = Tensor([]).to(alex3.device)\n",
        "    pred4 = Tensor([]).to(alex4.device)\n",
        "    true = Tensor([]).to(alex1.device)\n",
        "    loader = DataLoader(validation, batch_size=500, shuffle=False)\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        x = data[0].to(alex1.device)\n",
        "        y = data[1].to(alex1.device)\n",
        "        pred1 = torch.cat((pred1, Function.label_to_36_argmax(alex1.predict(x), device=alex1.device)), dim=0)\n",
        "        pred3 = torch.cat((pred3, Function.label_to_36_argmax(alex3.predict(x), device=alex3.device)), dim=0)\n",
        "        true = torch.cat((true, y), dim=0)\n",
        "\n",
        "    loader = DataLoader(validation_scaled, batch_size=500, shuffle=False)\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        x = data[0].to(alex1.device)\n",
        "        pred2 = torch.cat((pred2, Function.label_to_36_argmax(alex2.predict(x), device=alex1.device)), dim=0)\n",
        "        pred4 = torch.cat((pred4, Function.label_to_36_argmax(alex4.predict(x), device=alex3.device)), dim=0)\n",
        "    pred = pred1 * 1.2 + pred2 * 1.1 + pred3 + pred4\n",
        "\n",
        "    original = Path(TRAINED_MODELS_PATH / 'alex-nets' / '1way-depth(3, 4, 4)-fixmatch')\n",
        "    scaled = Path(TRAINED_MODELS_PATH / 'alex-nets-scaled' / '1way-depth(3, 4, 4)-fixmatch')\n",
        "    relabeled = Path(TRAINED_MODELS_PATH / 'alex-nets-relabeled' / '1way-depth(3, 4, 4)-fixmatch')\n",
        "    scaled_relabeled = Path(\n",
        "        TRAINED_MODELS_PATH / 'alex-nets-scaled-relabeled' / '1way-depth(3, 4, 4)-fixmatch')\n",
        "\n",
        "    alex1 = AlexNetClassifier(train, validation, **p1)\n",
        "    alex2 = AlexNetClassifier(train_scaled, validation_scaled, **p2)\n",
        "    alex3 = AlexNetClassifier(train_relabeled, validation_relabeled, **p3)\n",
        "    alex4 = AlexNetClassifier(train_scaled_relabeled, validation_scaled_relabeled, **p4)\n",
        "    alex1.load_network(original, 26)\n",
        "    alex2.load_network(scaled, 24)\n",
        "    alex3.load_network(relabeled, 29)\n",
        "    alex4.load_network(scaled_relabeled, 29)\n",
        "    pred1 = Tensor([]).to(alex1.device)\n",
        "    pred2 = Tensor([]).to(alex2.device)\n",
        "    pred3 = Tensor([]).to(alex3.device)\n",
        "    pred4 = Tensor([]).to(alex4.device)\n",
        "    true = Tensor([]).to(alex1.device)\n",
        "    loader = DataLoader(validation, batch_size=500, shuffle=False)\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        x = data[0].to(alex1.device)\n",
        "        y = data[1].to(alex1.device)\n",
        "        pred1 = torch.cat((pred1, Function.label_to_36_argmax(alex1.predict(x), device=alex1.device)), dim=0)\n",
        "        pred3 = torch.cat((pred3, Function.label_to_36_argmax(alex3.predict(x), device=alex3.device)), dim=0)\n",
        "        true = torch.cat((true, y), dim=0)\n",
        "\n",
        "    loader = DataLoader(validation_scaled, batch_size=500, shuffle=False)\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        x = data[0].to(alex1.device)\n",
        "        pred2 = torch.cat((pred2, Function.label_to_36_argmax(alex2.predict(x), device=alex1.device)), dim=0)\n",
        "        pred4 = torch.cat((pred4, Function.label_to_36_argmax(alex4.predict(x), device=alex3.device)), dim=0)\n",
        "    pred += pred1 + pred2*1.1 + pred3 + pred4\n",
        "\n",
        "\n",
        "\n",
        "    data_size = len(pred)\n",
        "    numbers = pred[:, :10]\n",
        "    letters = pred[:, 10:]\n",
        "    num_pred = torch.argmax(numbers, dim=1)\n",
        "    letter_pred = torch.argmax(letters, dim=1) + 10\n",
        "\n",
        "    output = torch.zeros((data_size, 36), dtype=torch.int, device=alex1.device)\n",
        "    output[range(data_size), num_pred] = 1\n",
        "    output[range(data_size), letter_pred] = 1\n",
        "    print(accuracy(output, true))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzGNtZl0OGLs"
      },
      "source": [
        "# obtain predictions on the test set\n",
        "def run_committee_test():\n",
        "    \"\"\"\n",
        "    shit code\n",
        "    don't do anything to this\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    training_set = read_train_labeled(DATASET_PATH)\n",
        "    validation_proportion = 0.1\n",
        "    validation, train = partition(training_set, [int(len(training_set) * validation_proportion)], False)\n",
        "    train_scaled = preprocess_scale_image(train)\n",
        "    train_scaled_relabeled = preprocess_260_labels(train_scaled)\n",
        "    train_relabeled = preprocess_260_labels(train)\n",
        "\n",
        "    validation = read_test(DATASET_PATH)\n",
        "    validation_scaled = preprocess_scale_image(validation)\n",
        "    validation_scaled_relabeled = preprocess_260_labels(validation_scaled)\n",
        "    validation_relabeled = preprocess_260_labels(validation)\n",
        "\n",
        "\n",
        "    p1 = {'n_way': 1, 'depth': (3, 4, 4), 'scaled': False, 'relabeled': False}\n",
        "    p2 = {'n_way': 1, 'depth': (3, 4, 4), 'scaled': True, 'relabeled': False}\n",
        "    p3 = {'n_way': 1, 'depth': (3, 4, 4), 'scaled': False, 'relabeled': True}\n",
        "    p4 = {'n_way': 1, 'depth': (3, 4, 4), 'scaled': True, 'relabeled': True}\n",
        "\n",
        "    original = Path(TRAINED_MODELS_PATH / 'alex-nets-rotation-augmented' / '1way-depth(3, 4, 4)')\n",
        "    scaled = Path(TRAINED_MODELS_PATH / 'alex-nets-rotation-augmented-scaled' / '1way-depth(3, 4, 4)')\n",
        "    relabeled = Path(TRAINED_MODELS_PATH / 'alex-nets-rotation-augmented-relabeled' / '1way-depth(3, 4, 4)')\n",
        "    scaled_relabeled = Path(TRAINED_MODELS_PATH / 'alex-nets-rotation-augmented-scaled-relabeled' / '1way-depth(3, 4, 4)')\n",
        "\n",
        "    alex1 = AlexNetClassifier(train, validation, **p1)\n",
        "    alex2 = AlexNetClassifier(train_scaled, validation_scaled, **p2)\n",
        "    alex3 = AlexNetClassifier(train_relabeled, validation_relabeled, **p3)\n",
        "    alex4 = AlexNetClassifier(train_scaled_relabeled, validation_scaled_relabeled, **p4)\n",
        "    alex1.load_network(original, 28)\n",
        "    alex2.load_network(scaled, 30)\n",
        "    alex3.load_network(relabeled, 7)\n",
        "    alex4.load_network(scaled_relabeled, 28)\n",
        "    pred1 = Tensor([]).to(alex1.device)\n",
        "    pred2 = Tensor([]).to(alex2.device)\n",
        "    pred3 = Tensor([]).to(alex3.device)\n",
        "    pred4 = Tensor([]).to(alex4.device)\n",
        "    loader = DataLoader(validation, batch_size=500, shuffle=False)\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        x = data.to(alex1.device)\n",
        "        pred1 = torch.cat((pred1, Function.label_to_36_argmax(alex1.predict(x), device=alex1.device)), dim=0)\n",
        "        pred3 = torch.cat((pred3, Function.label_to_36_argmax(alex3.predict(x), device=alex3.device)), dim=0)\n",
        "\n",
        "    loader = DataLoader(validation_scaled, batch_size=500, shuffle=False)\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        x = data.to(alex1.device)\n",
        "        pred2 = torch.cat((pred2, Function.label_to_36_argmax(alex2.predict(x), device=alex1.device)), dim=0)\n",
        "        pred4 = torch.cat((pred4, Function.label_to_36_argmax(alex4.predict(x), device=alex3.device)), dim=0)\n",
        "    pred = pred1*1.2 + pred2*1.1 + pred3 + pred4\n",
        "\n",
        "\n",
        "    original = Path(TRAINED_MODELS_PATH / 'alex-nets' / '1way-depth(3, 4, 4)-fixmatch')\n",
        "    scaled = Path(TRAINED_MODELS_PATH / 'alex-nets-scaled' / '1way-depth(3, 4, 4)-fixmatch')\n",
        "    relabeled = Path(TRAINED_MODELS_PATH / 'alex-nets-relabeled' / '1way-depth(3, 4, 4)-fixmatch')\n",
        "    scaled_relabeled = Path(\n",
        "        TRAINED_MODELS_PATH / 'alex-nets-scaled-relabeled' / '1way-depth(3, 4, 4)-fixmatch')\n",
        "\n",
        "    alex1 = AlexNetClassifier(train, validation, **p1)\n",
        "    alex2 = AlexNetClassifier(train_scaled, validation_scaled, **p2)\n",
        "    alex3 = AlexNetClassifier(train_relabeled, validation_relabeled, **p3)\n",
        "    alex4 = AlexNetClassifier(train_scaled_relabeled, validation_scaled_relabeled, **p4)\n",
        "    alex1.load_network(original, 26)\n",
        "    alex2.load_network(scaled, 24)\n",
        "    alex3.load_network(relabeled, 29)\n",
        "    alex4.load_network(scaled_relabeled, 29)\n",
        "    pred1 = Tensor([]).to(alex1.device)\n",
        "    pred2 = Tensor([]).to(alex2.device)\n",
        "    pred3 = Tensor([]).to(alex3.device)\n",
        "    pred4 = Tensor([]).to(alex4.device)\n",
        "    loader = DataLoader(validation, batch_size=500, shuffle=False)\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        x = data.to(alex1.device)\n",
        "        pred1 = torch.cat((pred1, Function.label_to_36_argmax(alex1.predict(x), device=alex1.device)), dim=0)\n",
        "        pred3 = torch.cat((pred3, Function.label_to_36_argmax(alex3.predict(x), device=alex3.device)), dim=0)\n",
        "\n",
        "    loader = DataLoader(validation_scaled, batch_size=500, shuffle=False)\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        x = data.to(alex1.device)\n",
        "        pred2 = torch.cat((pred2, Function.label_to_36_argmax(alex2.predict(x), device=alex1.device)), dim=0)\n",
        "        pred4 = torch.cat((pred4, Function.label_to_36_argmax(alex4.predict(x), device=alex3.device)), dim=0)\n",
        "    pred += pred1 + pred2*1.1 + pred3 + pred4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    data_size = len(pred)\n",
        "    numbers = pred[:, :10]\n",
        "    letters = pred[:, 10:]\n",
        "    num_pred = torch.argmax(numbers, dim=1)\n",
        "    letter_pred = torch.argmax(letters, dim=1) + 10\n",
        "\n",
        "    output = torch.zeros((data_size, 36), dtype=torch.int, device=alex1.device)\n",
        "    output[range(data_size), num_pred] = 1\n",
        "    output[range(data_size), letter_pred] = 1\n",
        "    # type cast to concatenated string\n",
        "    pred = output\n",
        "    pred = pred.detach().to('cpu').numpy().astype(int).astype(bytearray)\n",
        "    result = []\n",
        "    for i, row in enumerate(pred):\n",
        "        result.append([i, ''.join(row.astype(str))])\n",
        "    result = pd.DataFrame(result)\n",
        "    result.columns = [\"# ID\", \"Category\"]\n",
        "\n",
        "    # save predictions\n",
        "    if not SUBMISSION_PATH.exists():\n",
        "        SUBMISSION_PATH.mkdir(parents=True)\n",
        "    result.to_csv(SUBMISSION_PATH / (str(15) + '.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJqW1Vo2OTHn"
      },
      "source": [
        "run_committee_val()\n",
        "run_committee_test()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}